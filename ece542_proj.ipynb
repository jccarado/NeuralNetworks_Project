{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import anndata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have your gene expression data in 'X_train' (with cells as rows, genes as columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir('C:/Users/jccarado/Downloads/rotation_3')\n",
    "X_train = pd.read_csv('e17_expr.csv', index_col=0)\n",
    "cell_labels = pd.read_csv('e17_labels.csv', index_col=0, header=None)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(reconstructed, original):\n",
    "    # Cast both tensors to float32 to avoid type mismatch\n",
    "    reconstructed = tf.cast(reconstructed, tf.float32)\n",
    "    original = tf.cast(original, tf.float32)\n",
    "    return tf.reduce_mean(tf.square(reconstructed - original))\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    # Add a small epsilon to avoid log(0) issues\n",
    "    epsilon = 1e-10\n",
    "    p = tf.clip_by_value(p, epsilon, 1.0)\n",
    "    q = tf.clip_by_value(q, epsilon, 1.0)\n",
    "    return tf.reduce_sum(p * (tf.math.log(p) - tf.math.log(q)), axis=-1)\n",
    "\n",
    "def combined_loss(reconstructed, original, p_cluster, q_cluster, alpha=1.0, beta=1.0):\n",
    "    # Compute MSE loss\n",
    "    mse = mse_loss(reconstructed, original)\n",
    "    \n",
    "    # Compute KL Divergence loss\n",
    "    kl_loss = kl_divergence(p_cluster, q_cluster)\n",
    "    mse = tf.cast(mse, tf.float32)\n",
    "    kl_loss = tf.cast(kl_loss, tf.float32)\n",
    "    # Total loss is a weighted sum of MSE and KL divergence\n",
    "    return alpha * mse + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_stacked_autoencoder(input_shape, num_clusters):\n",
    "    # Encoder\n",
    "    input_data = layers.Input(shape=input_shape, name='input_layer')  # Input layer\n",
    "    \n",
    "    # Encoder layers (stacked)\n",
    "    x = layers.Dense(256, activation='relu')(input_data)  # First encoding layer        # Second encoding layer\n",
    "    latent_space = layers.Dense(128, activation='tanh')(x)  # Bottleneck layer (latent representation)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)            # Second decoding layer\n",
    "    reconstruction = layers.Dense(input_shape, activation='relu', name='reconstruction')(x)  # Output reconstruction\n",
    "    \n",
    "    # Clustering output (softmax for probabilities)\n",
    "    cluster_probs = layers.Dense(num_clusters, activation='softmax', name='clusters')(latent_space)  # Softmax output\n",
    "    \n",
    "    # Define the full model (encoder + decoder)\n",
    "    model = models.Model(inputs=input_data, outputs=[reconstruction, cluster_probs])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KLLossCallBack(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_train, cell_labels_numeric, encoder):\n",
    "        super(KLLossCallBack, self).__init__()\n",
    "        self.X_train = X_train\n",
    "        self.cell_labels_numeric = cell_labels_numeric\n",
    "        self.encoder = encoder\n",
    "        self.kl_values = []\n",
    "        self.p_dist = []\n",
    "        self.q_dist = []\n",
    "        self.cluster_labels = []\n",
    "        self.num_classes = len(np.unique(self.cell_labels_numeric))\n",
    "        self.num_classes_q = len(np.unique(self.cluster_labels))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get the encoded data after this epoch\n",
    "        encoded_data = self.encoder.predict(self.X_train)[0]\n",
    "        adata = anndata.AnnData(encoded_data)\n",
    "        sc.pp.pca(adata)\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, n_pcs=50)\n",
    "        sc.tl.louvain(adata)  # Run Louvain clustering on the encoded data\n",
    "        \n",
    "        self.cluster_labels = adata.obs['louvain']  # Extract cluster labels\n",
    "        self.num_classes_q = len(np.unique(self.cluster_labels))\n",
    "        # Compute KL Divergence Loss\n",
    "        self.p_dist = tf.one_hot(self.cluster_labels, depth=14, dtype=tf.float32)\n",
    "        self.q_dist = tf.one_hot(self.cell_labels_numeric, depth=14, dtype=tf.float32)\n",
    "        kl_score = kl_divergence(self.p_dist, self.q_dist)\n",
    "        kl_score = tf.reduce_mean(kl_score)\n",
    "        self.kl_values.append(kl_score)\n",
    "        # Optionally print KL at every epoch\n",
    "        print(kl_score)\n",
    "\n",
    "    def plot_kl(self):\n",
    "        # Plot ARI across epochs\n",
    "        plt.plot(range(1, len(self.kl_values) + 1), self.kl_values)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('KL Loss')\n",
    "        plt.title('KL Loss through Epochs')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from keras.callbacks import EarlyStopping\n",
    "import scanpy as sc\n",
    "import desc\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#X_train = X_train.T\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Build the autoencoder model as before\n",
    "\n",
    "# Use K-means to initialize the cluster labels\n",
    "\n",
    "\n",
    "encoder = create_stacked_autoencoder(input_dim, 14)\n",
    "adata = sc.AnnData(X_train)\n",
    "sc.pp.pca(adata, n_comps=50)\n",
    "sc.pp.neighbors(adata, n_neighbors=15, n_pcs=50)\n",
    "sc.tl.louvain(adata) \n",
    "\n",
    "# Train the model with the combined loss\n",
    "#autoencoder.compile(optimizer='adam', loss=lambda y_true, y_pred: clustering_loss(y_true, y_pred, encoded_data, cluster_labels, 14))\n",
    "cluster_labels = adata.obs['louvain'].astype(int)\n",
    "\n",
    "cell_type_counts = cell_labels.value_counts()\n",
    "sorted_cell_types = cell_type_counts.sort_values(ascending=False)\n",
    "print(\"GT: \", sorted_cell_types)\n",
    "# Ensure cluster_labels is a Pandas Series\n",
    "sorted_cluster_labels = pd.Series(cluster_labels).value_counts().sort_values(ascending=False)\n",
    "print(\"Prediction\", sorted_cluster_labels)\n",
    "cell_type_to_numeric = {cell_type: idx for idx, cell_type in enumerate(sorted_cell_types.index)}\n",
    "\n",
    "cell_type_to_numeric = {key[0]: value for key, value in cell_type_to_numeric.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getdims(x=(10000,200)):\n",
    "    \"\"\"\n",
    "    return the dims for network\n",
    "    \"\"\"\n",
    "    assert len(x)==2\n",
    "    n_sample=x[0]\n",
    "    if n_sample>20000:# may be need complex network\n",
    "        dims=[x[-1],128,32]\n",
    "    elif n_sample>10000:#10000\n",
    "        dims=[x[-1],64,32]\n",
    "    elif n_sample>5000: #5000\n",
    "        dims=[x[-1],32,16] #16\n",
    "    elif n_sample>2000:\n",
    "        dims=[x[-1],128]\n",
    "    elif n_sample>500:\n",
    "        dims=[x[-1],64]\n",
    "    else:\n",
    "        dims=[x[-1],16]\n",
    "    #dims=[x[-1],64,32] if n_sample>10000 else [x[-1],32,16]\n",
    "    return dims\n",
    "dims= getdims(adata.shape)\n",
    "print(dims)\n",
    "print(adata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cell_labels_numeric = cell_labels.iloc[:,0].map(cell_type_to_numeric)\n",
    "sorted_cell_types = cell_labels_numeric.value_counts().sort_values(ascending=False)\n",
    "print(cell_labels_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from keras import layers, models\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "cluster_labels_tf = tf.convert_to_tensor(cluster_labels)\n",
    "p_cluster = tf.one_hot(cluster_labels, depth=14, dtype=tf.float32)\n",
    "q_cluster = tf.one_hot(cell_labels_numeric, depth=14, dtype=tf.float32)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "kl_callback = KLLossCallBack(X_train, cell_labels_numeric, encoder=encoder)\n",
    "encoder.compile(optimizer=SGD(0.001, momentum=0.9), loss=lambda y_true, y_pred: combined_loss(y_true, y_pred, p_cluster, q_cluster, alpha=1.0, beta=1.0))\n",
    "#encoder.compile(optimizer=SGD(0.01, momentum=0.9), loss='mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.fit(X_train, [X_train, p_cluster], batch_size=32, epochs=100, callbacks=[kl_callback, early_stopping])\n",
    "kl_callback.plot_kl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "n_clusters = np.unique(cell_labels_numeric).shape[0]\n",
    "km = KMeans(n_clusters, n_init=20)\n",
    "y_pred = km.fit_predict(encoder.predict(X_train)[0])\n",
    "y = cell_labels_numeric\n",
    "print(tf.keras.losses.KLDivergence()(kl_callback.p_dist, kl_callback.q_dist))\n",
    "print ('K-means clustering result on extracted features: NMI =', normalized_mutual_info_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tf.keras.losses.KLDivergence()(kl_callback.p_dist, kl_callback.q_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "nmi_score = normalized_mutual_info_score(kl_callback.cell_labels_numeric, kl_callback.cluster_labels)\n",
    "ami_score = adjusted_mutual_info_score(kl_callback.cell_labels_numeric, kl_callback.cluster_labels)\n",
    "print(f\"Normalized Mutual Info: {nmi_score}\")\n",
    "print(f\"Adjusted Mutual Information: {ami_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "pca = PCA(n_components=50)  # You can change this value as needed\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2)  # We want to reduce it to 2D for visualization\n",
    "X_umap = umap_model.fit_transform(X_pca)\n",
    "cluster_labels = kl_callback.cluster_labels\n",
    "cluster_labels = cluster_labels.astype('category').cat.codes\n",
    "print(np.unique(cluster_labels))\n",
    "# Step 4: Visualize the UMAP projection\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cell_labels_numeric, cmap='tab20', s=10)\n",
    "plt.title('UMAP of Gene Expression Data (Ground Truth)')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.colorbar(label='Cell Type Label')\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c= cluster_labels, cmap='tab20', s=10)\n",
    "plt.title('UMAP of Gene Expression Data')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inertias = []\n",
    "for n in range(1, 50):  # Try values from 1 to 20\n",
    "    kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "    kmeans.fit(encoded_data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 50), inertias)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "for n in np.arange(0.1, 2.1, 0.1):  # test only from 5 to 15\n",
    "    sc.tl.louvain(adata, resolution=n) \n",
    "    cluster_labels = adata.obs['louvain'].astype(int) \n",
    "    silhouette_scores.append(silhouette_score(X_train, cluster_labels))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(silhouette_scores)\n",
    "plt.xlim(0.1,2.1)\n",
    "plt.xlabel('Resolution Parameter')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for Different resolutions')\n",
    "plt.show()\n",
    "\n",
    "print(silhouette_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
